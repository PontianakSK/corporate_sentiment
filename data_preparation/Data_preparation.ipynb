{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(name)s | %(levelname)s | %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('docs_sub_name_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up functions for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text patterns to be removed\n",
    "month_clause = '(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|Dec(ember)?)'\n",
    "\n",
    "regex_templates = {\n",
    "    # Date in the format\"Month day, Year / time /'\n",
    "    'reuters_date_time': '((('+ month_clause +'\\s+\\d{1,2},\\s+\\d{4})))\\s/\\s\\d{1,2}:\\d{1,2}\\s\\w{2}\\s/',\n",
    "    'another_reuters_regex': '(('+ month_clause +'\\s+\\d{1,2},\\s+\\d{4}))\\s\\/\\s\\d{1,2}:\\d{1,2}\\s\\w{2}\\s',\n",
    "    # Everything before /PRNewswire/: \"WHITE PLAINS, N.Y., May 23, 2018 /PRNewswire/ -- Bunge Limited (NYSE: BG) today \n",
    "    'cnbc_regex': '.*/PRNewswire/\\s--\\s',\n",
    "    # \"Updates N hours(minutes) ago\"\n",
    "    'updated_regex': '\\s?(Updated )?\\d{1,2} (hours|minutes|days) ago',\n",
    "    # in N minutes(hours)\n",
    "    'in_minutes': '\\s?in\\s\\d{1,2}\\s(minutes|hours|days)',\n",
    "    # Everything before GLOBE NEWSWIRE\n",
    "    'globe_newswire': '(.*)\\(GLOBE NEWSWIRE\\)\\s--',\n",
    "    # Another Date format - '13 April 2018 - '\n",
    "    'date_regex': '\\d{1,2}\\s'+ month_clause +'\\s\\d{4}\\s-\\s',\n",
    "    # Business Wire\n",
    "    'business_wire': '(.*)--\\(BUSINESS WIRE\\)--',\n",
    "    # COMMENTS at the beginning\n",
    "    'comments': '\\d{1,6}\\sCOMMENTS\\s',\n",
    "    # Place + date at the beginning\n",
    "    'place_date': '\\A(.*),\\s(('+ month_clause +'\\s+\\d{1,2},\\s+\\d{4}))\\s-',\n",
    "    # Updated an hour ago\n",
    "    'hour_ago': '\\A\\s?Updated an hour ago',\n",
    "    # N Min Read\n",
    "    'min_read': '\\s\\d{1,2} Min Read',\n",
    "    # time + in N minutes\n",
    "    'time_in_minutes': '\\s\\d{1,2}\\s\\w{2}\\s\\/\\sin\\s\\d{1,2}\\sminutes',\n",
    "    # by with time - By Phil Wahba 7:55 AM EST\n",
    "    'by_time': '\\A\\s?By\\s(.*)\\s\\d{1,2}:\\d{1,2}\\s\\w{2}\\s\\w{3}',\n",
    "    # by with date without the year with Reuters\n",
    "    'by_without_year': '\\A\\s?By\\s(.*)'+ month_clause +'\\s+\\d{1,2}\\s\\(Reuters\\)\\s-',\n",
    "    # by with year \n",
    "    'by_with_year': '\\A\\s?By\\s(.*)'+ month_clause +'\\s+\\d{1,2},\\s\\d{4}',\n",
    "    # City, date (Reuters)\n",
    "    'city_date_reuters': '\\A\\w+,\\s'+ month_clause +'\\s+\\d{1,2}\\s\\(Reuters\\)\\s-',\n",
    "    # a minute ago\n",
    "    'minute_ago': '\\A\\s?a minute ago',\n",
    "    # 10:30 AM ET Sun,\n",
    "    'time': '\\d{1,2}:\\d{2}\\s(\\w{2}\\s)*(Mon|Tue|Wed|Thu|Fri|Sat|Sun)?(,\\s)?',\n",
    "    # another date\n",
    "    'another_date': '(\\d{1,2}\\s)?('+ month_clause +')(\\s+\\d{1,2},)?\\s+\\d{2,4}',\n",
    "    # phone format\n",
    "    'phone_3': '(\\d+(/|-){1}\\d+(/|-){1}\\d{2,4})',\n",
    "    #phone with letter\n",
    "    'phone_with_letter': '1-800-\\d{3}-\\w{4}',\n",
    "    # just URL starting with HTTP(S)\n",
    "    'url_regex' : 'https?:\\/\\/(www\\.)?(\\S+)',\n",
    "    #View original content with multimedia: {URL}\n",
    "    'orig_content_with_multimedia' : 'View original content with multimedia: https?:\\/\\/.*\\n(.*)',\n",
    "    # View original content\n",
    "    'orig_content' : 'View original content: https?:\\/\\/.*\\n(.*)',\n",
    "    # Related Articles - URL\n",
    "    'related' : 'Related Articles\\nhttps?:\\/\\/.*',\n",
    "    # About Us: contact details\n",
    "    'about_us' : 'About Us:(\\n(.*))+',\n",
    "    # View on business wire\n",
    "    'view_on_bw' : 'View source version on businesswire\\.com :',\n",
    "    # For further information contact:\n",
    "    'for_further' : 'For further information contact:(\\n(.*))+',\n",
    "    # For more information visit:\n",
    "    'for_more_info' : 'For more information, visit (https?:\\/\\/)?(www\\.)?(\\S+)(\\s\\.)?',\n",
    "    # For additional information\n",
    "    'for_additional' : 'For additional information,(\\splease\\s)?visit (https?:\\/\\/)?(www\\.)?(\\S+)(\\s\\.)?',\n",
    "    # This press release features multimedia, view the full..\n",
    "    'press_release' : 'This press release features multimedia\\. View the full release here:\\s(https?:\\/\\/)?(www\\.)?(\\S+)(\\s\\.)?',\n",
    "    # For more information please\n",
    "    'for_more_info_please' : 'For more information,(\\splease)?\\svisit\\s(https?:\\/\\/)?(www\\.)?(\\S+)(\\s\\.)?',\n",
    "    #Email\n",
    "    'email' : '\\S*@\\S*\\s?',\n",
    "    # Visit at or follow\n",
    "    'visit_or_follow' : '((V|v)isit)(.*)(at)?follow us on(.*)',\n",
    "    # For more information\n",
    "    'for_more_info' : 'For more information[^,]+,\\svisit[^.]+.',\n",
    "    # Follow us on\n",
    "    'follow_us_on' : 'Follow us on[^.]+.',\n",
    "    # Follow us on Twitter\n",
    "    'follow_us_on_twitter' : 'follow us on Twitter(:)?\\s@[^\\s]+',\n",
    "    # TO learn more go to\n",
    "    'go_to' : 'Like (U|u)s on Facebook',\n",
    "    # LIke this story\n",
    "    'like_this_story' : 'Like this story?(.*)',\n",
    "    #phone\n",
    "    'phone' : '(\\+?\\s?\\d{1,3}\\s)?(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})',\n",
    "    #contact telephone email\n",
    "    'contact_telephone_email' : 'Contact:(.*)?(Date:)?(.*)?((Tele)?phone)?(.*)?Email:',\n",
    "    # Matches any URL\n",
    "    'url_all' : '((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\".,<>?«»“”‘’]))'\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding and replacing regex patterns in text\n",
    "\n",
    "def find_regex(text, template):\n",
    "    res = re.sub(template,  ' ', text)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial cleanup\n",
    "\n",
    "def cleanup_and_to_lower(text):\n",
    "    \"\"\"\n",
    "    1. remove extra white spaces (tabs, new lines, spaces, etc.)\n",
    "    2. bring text to lower case    \n",
    "    \"\"\"\n",
    "    text = \" \".join(text.split())\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "# There are two options available. 'remove_all_punct' is a default. Make changes to 'do_data_prep' function if necessary.\n",
    "\n",
    "def remove_all_punct(sentences):\n",
    "    \"\"\"\n",
    "    removes all punctuation except $\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sent in sentences:\n",
    "        punct=\"\\\"|\\';,[]:?!=%#()—\\\\/~*+<>@^{}-.“”‘’\"\n",
    "        for char in punct:\n",
    "            sent = sent.replace(char,' ')\n",
    "        new_sents.append(sent)\n",
    "    return new_sents\n",
    "\n",
    "def remove_some_punct(sentences):\n",
    "    \"\"\"\n",
    "    removes all punctuation except $ ! \" % “”\n",
    "    \"\"\"\n",
    "    new_sents = []\n",
    "    for sent in sentences:\n",
    "        punct=\"|;,\\'-[]:?=—#\\(\\)\\\\\\/~*+<>@^‘’\"\n",
    "        for char in punct:\n",
    "            sent = sent.replace(char,' ')\n",
    "        new_sents.append(sent)\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "\n",
    "def remove_stopwords(sentences):\n",
    "    new_sents = []\n",
    "    for sent in sentences:\n",
    "        sent_without_stopwords = [word for word in sent.split() if word not in stop_words]\n",
    "        new_sents.append(\" \".join(sent_without_stopwords))\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "# There are two options available. 'lemmatize_spacy' is a default. Make changes to 'do_data_prep' function if necessary.\n",
    "\n",
    "def lemmatize_spacy(sentences):\n",
    "    new_sents = []\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        lemmatized_sent = []\n",
    "        for token in sent:\n",
    "            lemmatized_sent.append(token.lemma_)\n",
    "        new_sents.append(\" \".join(lemmatized_sent))\n",
    "    return new_sents\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_nltk(sentences):\n",
    "    new_sents = []\n",
    "    for sent in sentences:\n",
    "        token_words=word_tokenize(sent)\n",
    "        token_words\n",
    "        lemmatized_sent=[]\n",
    "        for word in token_words:\n",
    "            lemmatized_sent.append(lemmatizer.lemmatize(word))\n",
    "        new_sents.append(\" \".join(lemmatized_sent))\n",
    "    return new_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def do_data_prep(df):\n",
    "    \"\"\"\n",
    "    1. removing unnecessary text patterns\n",
    "    2. removing extra white spaces (tabs, new lines, spaces, etc.) and bringing to lower case\n",
    "    3. tokenizing sentences\n",
    "    4. removing punctuation\n",
    "    5. removing stopwords\n",
    "    6. lemmatizing sentences\n",
    "    \"\"\"\n",
    "    logging.info('Creating a new column...')\n",
    "    df['prep_text'] = df.text_w_ids\n",
    "    \n",
    "    logging.info('Removing extra text patterns...')\n",
    "    for temp in regex_templates:\n",
    "        df.prep_text = df.prep_text.apply(lambda x: find_regex(x, regex_templates[temp]))\n",
    "        \n",
    "    logging.info('Removing extra spaces and bringing text to lower case...')    \n",
    "    df['prep_text'] = df.prep_text.apply(cleanup_and_to_lower)\n",
    "    \n",
    "    logging.info('Tokenizing text into sentences...')\n",
    "    df['prep_text'] = df.prep_text.apply(sent_tokenize)\n",
    "    \n",
    "    logging.info('Removing text punctuation...')\n",
    "    df['prep_text'] = df.prep_text.apply(remove_all_punct)\n",
    "    \n",
    "    logging.info('Removing stopwords...')\n",
    "    df['prep_text'] = df.prep_text.apply(remove_stopwords)\n",
    "    \n",
    "    logging.info('Performing lemmatization...')\n",
    "    df['prep_text'] = df.prep_text.apply(lemmatize_spacy)\n",
    "    \n",
    "    logging.info('Finished')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing data preparation and exporting results to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = do_data_prep(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('docs_preprocessed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
